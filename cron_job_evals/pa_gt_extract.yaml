dataset: PA_256_Sept_10
# dataset: Nvsept6new
generate_ground_truth: false 

extract_ground_truth: true

questions_generator:
   prompt: "default"
   num_questions_per_chunk: 1
   max_chunks: 10     # Maximum number of chunks to be used for question generation
   llm: dkubex
   llm_url: http://54.208.251.191:30001/v1/
   llmkey: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoia2VlcnRoYW5hLXBhcmltYWxsYSIsInR5cGUiOiJhcGkiLCJpZCI6Ii9kZXBsb3ltZW50L2xsYW1hM3dpdGg4a2RlcC8ifQ.MgB2Vw31UJBGjAIs6yEW4rGSyvJchHhKjQGwI5950wo
   max_tokens: 2048

ground_truth: /home/sai-bandela/ground_truth15.csv

rag_configuration: /home/sandeep-vangalapudi/cron_job_evals/rag_configs/pa_rag_qrw.yaml

vectorstore: weaviate_vectorstore
weaviate_vectorstore:
   url: ""
   auth_key: ""
   provider: dkubex
   properties:
   - paperdocs
   - dkubexfm

evaluator:
 - semantic_similarity_evaluator       # Vector Similarity
 - correctness_evaluator               # LLM Similarity
#  - answer_relevancy_evaluator
#   - retrieval_evaluator

semantic_similarity_evaluator:
   prompt: "default"
   metrics:
   - similarity_score

correctness_evaluator:
   prompt: |
       You are an expert evaluation system for a question answering chatbot.

       You are given the following information:
       - a user query, and
       - a generated answer

       You may also be given a reference answer to use for reference in your evaluation.

       Your job is to judge the relevance and correctness of the generated answer.
       Output a single score that represents a holistic evaluation.
       You must return your response in a line with only the score.
       Do not return answers in any other format.
       On a separate line provide your reasoning for the score as well.

       Please act as an impartial judge and evaluate the similarity of the response provided by
       an assistant against the reference answer for user question below.
       You will be given a reference answer and the assistant's answer.
       Your job is to compare the assistant's answer with the reference answer for similarity
       in the context of the user's question.
       Provide a score between 1 and 10 with an interval of 1
       Do not allow the length of the response to influence your evaluation.
       Be as objective as possible. Output your
   llm: dkubex
   llm_url: "http://3.87.157.251:30003/v1/"
   llmkey: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoic2FuZGVlcC12YW5nYWxhcHVkaSIsInR5cGUiOiJhcGkiLCJpZCI6Ii9kZXBsb3ltZW50L2xsYW1hc2t5LyJ9.L6iC5xwV0CjbNTVKO5DxeoRquMHGz9lpvpLI1KDUWs4"
   max_tokens: 2048

# cross_vector_similarity_evaluator:
#     prompt: "default"
#     llm: openai         # dkubex
#     llmkey: "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" # <llmkey?
#     llm_url: ""             # http://54.179.35.250:30002/v1/
#     max_tokens: 2048
#     rag_configuration: "/home/sai-bandela/simple_rag.yaml"

# answer_relevancy_evaluator:
#    prompt: "default"
#    llm: dkubex
#    llm_url: "http://35.153.80.74:30002/v1/"
#    llmkey: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoic2hydXRoaS1zcmVlIiwidHlwZSI6ImFwaSIsImlkIjoiL2RlcGxveW1lbnQvbGFtYTMxOGsvIn0.StHl2SRpcIYUo-FGExVELoKB1SsD208clwObnb0v0vQ"   max_tokens: 2048

retrieval_evaluator:
   weaviate_vectorstore:
       kind: weaviate
       vectorstore_provider: dkubex
       textkey: paperdocs
       # embedding_class: HuggingFaceEmbedding               # Use 'HuggingFaceEmbedding' for embedding models from HuggingFace, or 'OpenAIEmbedding' for OpenAI embeddings
       # embedding_model: "BAAI/bge-large-en-v1.5"           # Embedding model name
       ### dkubex embedding config
       embedding_provider: "sky"
       embedding_url: http://35.91.254.178:30001/v1/
       embedding_key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoib2NkbGdpdCIsInR5cGUiOiJhcGkiLCJpZCI6Ii9kZXBsb3ltZW50L2JnZWxyZy8ifQ.Ljt4IdI7MPtK2M9hdQ5Cu3ZVQnCEvNZFbM69CowVuek
       llmkey: ""                                          # API key for the embedding model (if required)
       similarity_top_k: 3
   metrics:
   - mrr
   - hit_rate
 
groundtruth_extractor:
   date_start: "2024-10-24"
   date_end: "2024-10-24"
   email: admin@dkubex.ai
   password: noB3y8K9VC0tmMgG
   # password: dkubex123 # password
   deployment_url: "http://3.87.157.251:30003/v1/"
   deployment_key: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoic2FuZGVlcC12YW5nYWxhcHVkaSIsInR5cGUiOiJhcGkiLCJpZCI6Ii9kZXBsb3ltZW50L2xsYW1hc2t5LyJ9.L6iC5xwV0CjbNTVKO5DxeoRquMHGz9lpvpLI1KDUWs4"
   state: PA
   filter_user: ''
   no_of_rows:
   # supabase_url: http://supabase-kong.securellm:8000
   # sllmbase_url: http://securellm-fe.securellm:3000/securellm
   supabase_url: https://devbot.ginicechat.com/supabase
   sllmbase_url: https://devbot.ginicechat.com/securellm
   supabase_key: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.ewogICAgInJvbGUiOiAiYW5vbiIsCiAgICAiaXNzIjogInN1cGFiYXNlIiwKICAgICJpYXQiOiAxNjc1NDAwNDAwLAogICAgImV4cCI6IDE4MzMxNjY4MDAKfQ.ztuiBzjaVoFHmoljUXWmnuDN6QU2WgJICeqwyzyZO88"
   extract_all: True # if True: <will extract all the rows> else <will extract only feedbacked rows.>
   dev_team: 
   #- "help"
   - "sagar-suman"
   - "vedant-acharya"
   # - "udai-kiran"
   # - "hemanth-ravi"
   # - "kiran-vijapure"
   # - "songole"
   # - "vivek"
   # - "nice"

tracking:
   experiment: "Eval_PA_2024-10-24"
